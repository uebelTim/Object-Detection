{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of how to use the general Machine Learning Functions in general_ML_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the script in which you want to import general_ML_utils has to be located in the same root folder as general_ML_utils.\n",
    "\n",
    "Then import it by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_ML_utils as gmlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a dataset to train a model, the data must be in a specific form and format. A function that uses a folder of images and labels and constructs a usable dataset is often called a 'pipeline'. \n",
    "\n",
    "Such a pipeline is the function build_img_dataset_pipeline().\n",
    "\n",
    "The function builds a pipeline that formats the images and labels and splits the dataset into training and validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try the function we first have to have a dataset. For that we download the popular Fashion MNIST Dataset https://github.com/zalandoresearch/fashion-mnist?tab=readme-ov-file\n",
    "\n",
    "We will download the images and save them to a folder in the root directory of this script. We only use the 10.000 images of the test set for faster execution. Normally this is not enough. A rough estimate is at least 1000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we download the fashion mnist dataset from keras\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "#concatenate the training and testing data because we will split it later\n",
    "# images = np.concatenate((train_images, test_images))\n",
    "# labels = np.concatenate((train_labels, test_labels))\n",
    "images = test_images\n",
    "labels = test_labels\n",
    "#create a folder to store the images\n",
    "os.makedirs('fashion_mnist', exist_ok=True)\n",
    "os.makedirs('fashion_mnist/images', exist_ok=True)\n",
    "os.makedirs('fashion_mnist/labels', exist_ok=True)\n",
    "print('Saving images...')\n",
    "for i, image in tqdm(enumerate(images)):\n",
    "    Image.fromarray(image).save('fashion_mnist/images/' + str(i) + '.png')\n",
    "print('Done!')\n",
    "print('Images saved in fashion_mnist/images')\n",
    "#one hot encode the labels\n",
    "labels = gmlu.one_hot_encode(labels, 10)\n",
    "#make dataframe with first column the image names and the other columns the labels, each column corresponding to a class\n",
    "image_names = [f'{i}.png' for i in range(len(images))]\n",
    "data = np.column_stack([image_names, labels])\n",
    "columns = ['image_name'] + [f'class_{i}' for i in range(10)]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "# Save the DataFrame as a CSV file, this will be the labels file\n",
    "df.to_csv('fashion_mnist/labels.csv', index=False)\n",
    "print('Labels saved as fashion_mnist/labels.csv')\n",
    "print('The first 5 rows of the labels file are:')\n",
    "print(df.head())\n",
    "train_images, test_images, train_labels, test_labels, images, labels = None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the raw dataset in the right form.\n",
    "\n",
    "The images are saved in the folder fashion_mnist/images and the labels a saved in the file fashion_mnist/labels.csv.\n",
    "\n",
    "The images are of size 28x28 and are greyscale. There are 10 classes.\n",
    "\n",
    "We can now use the build_img_dataset_pipeline() of the gmlu function.\n",
    "\n",
    "To see the documentation of a function with all possible parameters we can call help(function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.build_img_dataset_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the function. Since the function is saved in the module general_ML_utils.py which we imported as gmlu we call the function with gmlu.build_img_dataset_pipeline().\n",
    "\n",
    "We use the function with augmentation_strength=0.3 for every augmentation except vertical and horizontal translation since all objects in the dataset are perfectly centered. This means that we randomly adjust brightness and contrast by up to 30% of the original value and add gaussian noise with up to 0.3 as stadard deviation (the exact value of the augmentation is random and changes each epoch. The augmentation_strength only represents the maximum possible value). We also flip images randomly vertically and horizontally.\n",
    "\n",
    "augmentation_strength=0 would mean that we only use the original images, but augmentation always leads to betters results. Normal values are 0.1 or 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "train_ds, val_ds, test_ds = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(32, 32, 1), n_classes=10, img_type='png',augment_strength=[1, 0, 0, 0.3, 0.3, 0.3],seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function the augmentations get applied dynamically. When you want to use the augmented images in a different software or programming language we can save the augmented dataset to a folder with the save_augmented_dataset() function.\n",
    "\n",
    "With this function we copy the original images plus the augmented images to a new folder and generate the corresponding labels.\n",
    "\n",
    "augment_iterations=2 means that we create 2 randomly augmented versions from each original image.\n",
    "\n",
    "The original images keep their name and to the augmented images a '_aug' plus the iteration number is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "save_path = r'fashion_mnist/augmented'\n",
    "gmlu.save_augmented_dataset(img_path=img_path, label_path=label_path, save_path=save_path, augment_strength=0.1, augment_iterations=2, img_size=(32, 32, 1),img_type='png', n_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 1\n",
    "\n",
    "1. Build a dataset with a train and val set of equal sizes and no test set and apply only the augmentations of brightness and gaussian noise with strength 0.9.\n",
    "\n",
    "2. Build a dataset once with a halfed image-resolution ond once with a four time higher image-resolution and compare the results.\n",
    "\n",
    "3. Build a dataset consisting of only a test set and name the first 20 image-filenames. Compare them when shuffling and not shuffling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the build_model() function to compile a model for use. \n",
    "\n",
    "We can choose from several different model architectures which are: 'resnet', 'inceptionv3', 'mobilenetv2', 'xception','efficientnet'. The models can be constructed in the sizes 's', 'm', 'l'. Bigger models usually yield a better performance but this difference is small for small and easy datasets and increases the needed training time drastically.\n",
    "\n",
    "We use the mobilenet architecture here because it is fast to train and leads to good results most of the time.\n",
    "\n",
    "Each model can be loaded with pretrained weights, which were trained on the imagenet dataset. Using pretrained weights often leads to a better performance, but might have slightly negative effects when the used dataset is very different from the imagenet dataset. The weights cant be used if the images are greyscale. So we wont use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gmlu.build_model(input_shape=(32, 32, 1), n_classes=10, model_type='simplenet',model_size='s',weights=None, classification_type='multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 2\n",
    "\n",
    "1. Build a EfficientNet in size s and count the number of Conv2D Layers. Compare with simpleNet in size L.\n",
    "\n",
    "2. Build a s-sized simplenet with 4 classification layers with 128, 64, 32 and 16 neurons. Compare the total number of parameters with the same net but with 2 classification layers with 256 and 32 neurons.\n",
    "\n",
    "3. Try to build a xception model with input_shape=(32,32,1). What is the result?\n",
    "\n",
    "4. Try to build a resnet model with input_shape=(32,32,1) and use the imagenet weights. What is the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the compiled model with the dataset.\n",
    "\n",
    "We wont get a very good result because the dataset is so small, but that enables a much faster training.\n",
    "\n",
    "During the training we see the metrics accuracy, precision, recall and f1-score for each epoch. After each epoch we see all metrics for the validation set.\n",
    "\n",
    "The model the function returns is the model that was saved during the epoch with the best f1-score of the valodation set. The history is a object that contains the values of all metrics for all epochs. It is used to plot the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'fashion_mnist/models'\n",
    "model, history = gmlu.train_model(model=model, train_ds=train_ds, val_ds=val_ds, save_path=save_path, folder='demonstration', epochs=10, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the history object to plot the course of al metrics with the plot_history() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmlu.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load a trained model directly via the `load_model_from_path`-function. These 3 methods to load a trained model are equivalent. The last line only loads the weights and requires that the fitting model architecture for the weights was build beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gmlu.load_model_from_path(r'fashion_mnist\\models\\demonstration\\simplenet\\best_model')\n",
    "model = gmlu.load_model_from_path(r'fashion_mnist\\models\\demonstration\\simplenet\\best_model.h5')\n",
    "model.load_weights(r'fashion_mnist\\models\\demonstration\\simplenet\\best_model_weights\\checkpoint.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 3\n",
    "\n",
    "1. Follow the value of lr (learning rate) over the epochs. What causes it to change?\n",
    "\n",
    "2. Follow the metrics for the train set and the val set. DId the best epoch for the train set correspond to the best epoch for the val set?\n",
    "\n",
    "3. Train a simplenet size m multilabel model on the bee_dataset for 10 epochs with img_size 100x50\n",
    "\n",
    "4. Build a efficientnet size s and a simplenet size l. Train both of them for 2 epochs and compare the time per epoch.\n",
    "\n",
    "5. Build the dataset with image_size=(128,128,1), train the simplenet size s model with it for 10 epochs and compare the training times and the best metrics.\n",
    "\n",
    "6. Look into the 'savedir' directory. What is saved there and what does it contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to use the trained model to predict images. For that we use the make_predictions() function.\n",
    "\n",
    "We pass the model directly to that function. Or we pass the path to the saved model. \n",
    "\n",
    "Then we provide the data we want to predict. We use the test set, but we can also pass a single image or a path to a image fole or a path to a folder with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.make_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "train_ds, val_ds, test_ds = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(32, 32, 1), n_classes=10, img_type='png',augment_strength=[1, 0, 0, 0.3, 0.3, 0.3],seed=42,verbose=0)\n",
    "model = gmlu.load_model_from_path(r'fashion_mnist\\models\\demonstration\\simplenet\\best_model')\n",
    "\n",
    "probabilities, predictions = gmlu.make_predictions(model=model, data=test_ds, img_size=(32,32,1),classification_type='multiclass')\n",
    "print('probabilities:\\n', probabilities)\n",
    "print('predictions:\\n', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 4\n",
    "\n",
    "1. Predict all the images in the path fashion_mnist/images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of a model we can use the evaluate_model function. This function calculates the metrics of the model on a test dataset. It calculates the metric on the whole dataset and per class and displays them in a bar plot. It also generates the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.evaluate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the build_img_dataset_pipeline() function to generate the test dataset, it is important to use the same seed as used when training the model. Here we use seed=42 in both locations. This ensures that the data is shuffeled in the same order and the images in the train, val and test sets are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "train_ds, val_ds, test_ds = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(32, 32, 1), n_classes=10, img_type='png',seed=42,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'fashion_mnist\\models\\demonstration\\simplenet\\best_model'\n",
    "class_names=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "report_dict = gmlu.evaluate_model(model= model_path, test_ds=test_ds,classification_type='multiclass',class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 5\n",
    "\n",
    "1. Identify the class that is most often mistaken for a different class. What can you say about the precision and recall values of that class?\n",
    "\n",
    "2. What does a horizontal class spread in the confusion matrix indicate? A low precision or a low recall?\n",
    "\n",
    "3. What does a vertical class spread in the confusion matrix indicate? A low precision or a low recall?\n",
    "\n",
    "4. Why is the accuracy the same for all classes?\n",
    "\n",
    "5. Evaluate the model for the bee dataset with a confidence threshold of 0.7 and 0.3 and compare which metrics change the most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best confidence threshold values for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a multilabel problem we can optimize the individual confidence thresholds per class with the `optimize_conf_thresholds`-function. This optimization has no effect on multiclass problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.optimize_conf_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'bee_dataset\\images'\n",
    "label_path = r'bee_dataset\\labels.csv'\n",
    "train_ds,val_ds,_ = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(100, 50, 3), n_classes=4,val_size=0.3, test_size=0, img_type='jpeg', augment_strength=[1,0,0,0.1,0.1,0.1],seed=42,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next cell you need to do Übung 3.3 where you create a model for the multilabel bee-dataset, since the optimization of confidence thresholds is only applicable to multilabel models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_model_path =r'bee_dataset\\models\\demonstration\\simplenet\\best_model'\n",
    "report = gmlu.evaluate_model(model=bee_model_path, test_ds=val_ds,classification_type='multilabel')\n",
    "print(report['weighted avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_model_path =r'bee_dataset\\models\\demonstration\\simplenet\\best_model'\n",
    "optimal_thresholds = gmlu.optimize_conf_thresholds(model=bee_model_path, val_ds=val_ds)\n",
    "print('optimal_thresholds:', optimal_thresholds)\n",
    "print('len optimal_thresholds:', len(optimal_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = gmlu.evaluate_model(model=bee_model_path, test_ds=val_ds, conf_thresholds=optimal_thresholds, classification_type='multilabel')\n",
    "print(report['weighted avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 6\n",
    "\n",
    "1. Optimize the thresholds once for Recall and for Precision and compare the evaluation-results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal model for a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a model can vary strongly for different datasets. So if we want the best possible model for specific dataset we want to do a hyperparameter search with find_best_mode().\n",
    "\n",
    "This compares all the different model architectures with many different configurations to find the best model.\n",
    "\n",
    "First we build our datasets again, with img_size=(32,32,1). No model can use images with a smaller size except for simplenet with size 's', which can work with images of a minimal size of 8x8. To include all models in the search we have to use a image size of at least 75x75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.find_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "train_ds, val_ds, test_ds = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(32, 32, 1), n_classes=10, img_type='png',augment_strength=[1,0,0,0.1,0.1,0.1],verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start the hyperparameter search. This Function will run a long time. We set the maximal training epochs per model to 30 to reduce the training time. Ideally the epochs should be 20-60 (dependent on the complexity of the images and the number of classes). The `iterations` parameter sets the number of different hyperparameter-configurations that are tried. This should be minimal 20 but ideally more. This is because the used optimization algorithm optimizes the choice of hyperparameters and needs at least 20 iterations to work as intended (the optimization algorithm is a Tree-structured Parzen Estimator in combination with a Hyperband pruner). If we use less iterations than 20 then we have effectiveley a random search algorithm.\n",
    "\n",
    "The training progress of the individual iterations is logged in a online dashboard from the `Weights & Biases` library. When you click on the link at \"View run at :\" you can see live charts of the different training metrics. When you navigate to the root project of the run or click on the link at \"View project at\" you see the charts of the different metrics of all trials together. This is usefull to quickly see which iteration was the best or to get a sense of which model architecture performs the best for that dataset across different hyperparameter configurations.\n",
    "\n",
    "You dont need a account to see the dashboard and all logged data gets deleted after 7 days (but you can claim the data when you make a account, then it is permanent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'fashion_mnist/models/hyperparameter_search'\n",
    "gmlu.find_best_model(train_ds=train_ds, val_ds=val_ds, test_ds=test_ds, save_path=save_path, max_epochs=10, iterations=20, classification_type='multiclass', models='all', model_sizes=['s','m'],eval_metric='f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 7\n",
    "\n",
    "1. Go to the wandb dashboard and group the runs once by model_size and by model. What can you generally say about the performance?\n",
    "\n",
    "2. Go to the wandb dashboard and list the 5 most important parameters and if they contribue positively or negatively.\n",
    "\n",
    "3. Select one run and go to the logs. Can you find the save path of the trained model and the Host-PC?\n",
    "\n",
    "4. Start a new hyperparameter search with all models and a specify a project name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the performance of a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the performance of a model we use a technique called cross-validation. We need to do this because the evaluation and training metrics of a model (like val_f1_score) dont tell the whole story, because that is only the performance on a small part of the dataset. Per chance the test or validation sets could only consist of the easiest to classify images, which would lead to the model underperforming in the production environment. \n",
    "\n",
    "Cross-validation gives a much better performance indication of the model by splitting the dataset into multiple parts where each part is used once as the test set and the other parts as the train set. By this the model gets tested on the whole dataset not only on a part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will evaluate the first model, that we trained before. It is the MobileNet with the standart configuration. That the model was trained is irrelevant since we retrain the model in the cross-validation. We train the model here only for 10 epochs in 5 folds to make it faster. Standard parameters would be ca. 60 epochs and 5 folds.\n",
    "\n",
    "Normally one would use the cross-validation to evaluate the performance of a Hyperparameter configuration that was derived by a hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "model_path= r'fashion_mnist\\models\\demonstration\\simplenet\\best_model'\n",
    "save_path = r'fashion_mnist\\cross_validation'\n",
    "\n",
    "gmlu.cross_validation(model=model_path, img_path=img_path, label_path=label_path, save_path=save_path, folds = 2, epochs = 1, n_classes=10, classification_type='multiclass', img_type='png', augment_strength=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übungen 8\n",
    "\n",
    "1. Do a cross validation and save the filenames of the folds. Name the first 10 filenames that are used in the first and second fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.export_onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export a model to the ONNX-format from a path of a saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= r'fashion_mnist\\models\\demonstration\\simplenet\\best_model'\n",
    "save_path =  r'fashion_mnist\\models\\demonstration\\ONNX'\n",
    "gmlu.export_onnx_model(model_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can directly pass a build model. \n",
    "When no save-path is provided then the onnx-model will be saved in a standard location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gmlu.load_model_from_path(model_path)\n",
    "gmlu.export_onnx_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "img_path = r'fashion_mnist/images'\n",
    "label_path = r'fashion_mnist/labels.csv'\n",
    "train_ds, val_ds, test_ds = gmlu.build_img_dataset_pipeline(img_path=img_path, label_path=label_path, img_size=(32, 32, 1), n_classes=10, img_type='png',augment_strength=[1,0,0,0.1,0.1,0.1],verbose=0)\n",
    "\n",
    "model = onnx.load(r'fashion_mnist\\models\\demonstration\\ONNX\\simplenet\\model.onnx')\n",
    "# Create an ONNX Runtime session\n",
    "session = ort.InferenceSession(model.SerializeToString())\n",
    "# Get the input name(s) of the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "# Get the output name(s) of the model\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "tf_model = gmlu.load_model_from_path(r'fashion_mnist\\models\\demonstration\\simplenet\\best_model')\n",
    "for batch in test_ds:\n",
    "    images, labels = batch\n",
    "    # Get the data as a numpy array\n",
    "    images = images.numpy()\n",
    "    images = images.astype(np.float32)\n",
    "    # Run the prediction\n",
    "    prediction = session.run([output_name], {input_name: images})[0]\n",
    "    print('predictions of first batch:\\n',prediction[0])\n",
    "    tf_prediction = tf_model.predict(images)\n",
    "    print('compare with predictions of tensorflow model:\\n',tf_prediction[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have two modalities of the same objects we can combine the predictions of the modalities to get a better result than using only one. \n",
    "\n",
    "For multimodal prediction there are two possibilities: batch prediction and single image prediction\n",
    "\n",
    "When using batch prediction we are only interested what classes are generally present in the batches and not at what position in the batch they are positioned. This is usefull when there are many images of the same object and we want to know if there is a defect present anywhere. Or generally when the images can be grouped to larger categories (like Laufmeter in DIS project 149). For that we use the gmlu.multimodal_prediction_batch() function.\n",
    "\n",
    "When there is a perfect parity per image so that we want to combine only the predictions of the two images of the modalities we can use the gmlu.multimodal_prediction_single() function. This requires that there is the same number of images in the batches of the modalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multimodal combination on batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.multimodal_prediction_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the two models that should be used for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model_path = r'models/x_model'\n",
    "o_model_path = r'models/o_model'\n",
    "x_model = gmlu.load_model_from_path(x_model_path)\n",
    "o_model = gmlu.load_model_from_path(o_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the data for the multimodal prediction. We load three batches of images and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_path = 'Rolle_01935'\n",
    "x_batches = []\n",
    "o_batches = []\n",
    "labels=[]\n",
    "folders = os.listdir(batches_path)\n",
    "for folder in folders:\n",
    "    x_files = os.listdir(os.path.join(batches_path, folder, 'XDIS'))\n",
    "    o_files = os.listdir(os.path.join(batches_path, folder, 'ODIS'))\n",
    "    x_files = [os.path.join(batches_path, folder, 'XDIS', file) for file in x_files]\n",
    "    o_files = [os.path.join(batches_path, folder, 'ODIS', file) for file in o_files]\n",
    "    x_batch = [np.array(Image.open(file).convert('L').resize((128,128), Image.LANCZOS)) for file in x_files]\n",
    "    x_batch = np.expand_dims(x_batch, axis=-1)\n",
    "    print('x_batch size: ',np.shape(x_batch))\n",
    "    x_batches.append(x_batch)\n",
    "    \n",
    "    o_batch = [np.array(Image.open(file).convert('L').resize((128,128), Image.LANCZOS)) for file in o_files]\n",
    "    o_batch = np.array(o_batch)\n",
    "    o_batch = np.expand_dims(o_batch, axis=-1)\n",
    "    print('o_batch size: ',np.shape(o_batch))   \n",
    "    o_batches.append(o_batch)   \n",
    "    \n",
    "    label_file = os.path.join(batches_path, folder, 'labels', 'classes.csv')\n",
    "    label = pd.read_csv(label_file, header=None)\n",
    "    labels.append(label.values[0].tolist())\n",
    "    print('label: ',label.values.tolist())\n",
    "print('nr of x_batches: ',len(x_batches))\n",
    "print('nr of o_batches: ',len(o_batches))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function gmlu.multimodal_prediction_batch only takes in a single batch of data per modality and produces a single prediction and a single probability vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, probs,_,_ = gmlu.multimodal_prediction_batch(x_model, o_model, x_batches[0], o_batches[0], n_classes=10,classification_type='multilabel')\n",
    "print('preds: ',preds)\n",
    "print('probs: ',probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to predict many batches we predict them in a loop and save the predictions in a list.\n",
    "\n",
    "To evaluate the prediction we can use the gmlu.evaluate_predictions() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gmlu.evaluate_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(x_batches)):\n",
    "    preds, probs,_,_ =gmlu.multimodal_prediction_batch(x_model, o_model, x_batches[i], o_batches[i], n_classes=10,classification_type='multilabel')\n",
    "    predictions.append(preds)\n",
    "    \n",
    "print('preds: ',predictions)\n",
    "predictions = gmlu.one_hot_encode(predictions, 10, classification_type='multilabel')\n",
    "print('predictions: ',predictions)\n",
    "print('labels: ',labels)\n",
    "report = gmlu.evaluate_predictions(predictions, labels, classification_type='multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_Ts, optimal_thresholds = gmlu.optimize_multimodal_parameters(x_model, o_model, x_batches, o_batches, labels, n_classes=10, classification_type='multilabel',type='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal_Ts: ',optimal_Ts)\n",
    "print('optimal_thresholds: ',optimal_thresholds)\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(x_batches)):\n",
    "    preds, probs,_,_ =gmlu.multimodal_prediction_batch(x_model, o_model, x_batches[i], o_batches[i], n_classes=10,classification_type='multilabel',conf_thresholds=optimal_thresholds, T=optimal_Ts)\n",
    "    predictions.append(preds)\n",
    "    \n",
    "print('preds: ',predictions)\n",
    "predictions = gmlu.one_hot_encode(predictions, 10, classification_type='multilabel')\n",
    "print('predictions: ',predictions)\n",
    "print('labels: ',labels)\n",
    "report = gmlu.evaluate_predictions(predictions, labels, classification_type='multilabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multimodal combination of individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = tf.keras.datasets.cifar10.load_data()\n",
    "#split datasets into images and labels\n",
    "train_images, train_labels = train\n",
    "#split images into channels\n",
    "train_images1 = train_images[:,:,:,0:1]\n",
    "train_images2 = train_images[:,:,:,1:2]\n",
    "train_labels = gmlu.one_hot_encode(train_labels, 10)\n",
    "val_images, val_labels = val\n",
    "val_images1 = val_images[:,:,:,0:1]\n",
    "val_images2 = val_images[:,:,:,1:2]\n",
    "val_labels = gmlu.one_hot_encode(val_labels, 10)\n",
    "print('shape images1: ',np.shape(train_images1))\n",
    "print('shape images2: ',np.shape(train_images2))\n",
    "print('shape labels: ',np.shape(train_labels))\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'cifar10/models'\n",
    "model1 = gmlu.build_model(input_shape=(32, 32, 1), n_classes=10, model_type='simplenet',model_size='s', classification_type='multiclass',classifier_neurons=[256,64])\n",
    "model1, history1 = gmlu.train_model(model=model1, train_ds=(train_images1,train_labels), val_ds=(val_images1,val_labels), save_path=save_path, folder='channel1', epochs=10, patience=10)\n",
    "\n",
    "model2 = gmlu.build_model(input_shape=(32, 32, 1), n_classes=10, model_type='simplenet',model_size='s', classification_type='multiclass',classifier_neurons=[256,64])\n",
    "model2, history2 = gmlu.train_model(model=model2,  train_ds=(train_images2,train_labels), val_ds=(val_images2,val_labels), save_path=save_path, folder='channel2', epochs=10, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gmlu.load_model_from_path(r'cifar10\\models\\channel1\\simplenet\\best_model')\n",
    "model2 = gmlu.load_model_from_path(r'cifar10\\models\\channel2\\simplenet\\best_model')\n",
    "preds, probs,_,_ =gmlu.multimodal_prediction_single(model1, model2, val_images1, val_images2, n_classes=10, classification_type='multiclass',T=1000)\n",
    "predictions = gmlu.one_hot_encode(preds, 10, classification_type='multiclass')\n",
    "report = gmlu.evaluate_predictions(predictions, val_labels, classification_type='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_Ts, optimal_thresholds = gmlu.optimize_multimodal_parameters(model1, model2, val_images1, val_images2, val_labels, n_classes=10, classification_type='multiclass',type='single')\n",
    "preds, probs,_,_ =gmlu.multimodal_prediction_single(model1, model2, val_images1, val_images2, n_classes=10, classification_type='multiclass',T=optimals_Ts)\n",
    "predictions = gmlu.one_hot_encode(preds, 10, classification_type='multiclass')\n",
    "report = gmlu.evaluate_predictions(predictions, val_labels, classification_type='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs1, preds1 = gmlu.make_predictions(model1, val_images1, classification_type='multiclass')\n",
    "print('preds1: ',preds1[:10])\n",
    "predictions1 = gmlu.one_hot_encode(preds1, 10, classification_type='multiclass')\n",
    "report1 = gmlu.evaluate_predictions(predictions1, val_labels, classification_type='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs2, preds2 = gmlu.make_predictions(model2, val_images2, classification_type='multiclass')\n",
    "print('preds2: ',preds2[:10])\n",
    "predictions2 = gmlu.one_hot_encode(preds2, 10, classification_type='multiclass')\n",
    "report2 = gmlu.evaluate_predictions(predictions2, val_labels, classification_type='multiclass')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
