{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(\"tf.__version__:\", tf.__version__)\n",
    "print(\"keras.__version__:\", keras.__version__)\n",
    "print(\"keras_cv.__version__:\", keras_cv.__version__)\n",
    "import keras_hub\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_cv\n",
    "print(keras_cv.models.RetinaNet.presets.keys())\n",
    "print(keras_cv.models.YOLOV8Detector.presets.keys())\n",
    "print(keras_cv.models.EfficientNetLiteBackbone.presets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone = keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_s_backbone_coco\")\n",
    "# yolo = keras_cv.models.YOLOV8Detector(\n",
    "#     num_classes=20,\n",
    "#     bounding_box_format=\"xyxy\",\n",
    "#     backbone=backbone,\n",
    "#     fpn_depth=1,\n",
    "# )\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001,global_clipnorm=10,)\n",
    "# yolo.compile(optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import keras_cv\n",
    "import keras\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def build_image_detection_pipeline(img_path=None, labels_path=None, annotation_format=\"yolo\", \n",
    "                                  img_size=None, batch_size=32, augment_strength={'rotation':0.1,\n",
    "                                                                                       'shear':0.1,\n",
    "                                                                                       'hue':0.1,\n",
    "                                                                                       'saturation':0.1,\n",
    "                                                                                       'contrast':0.1,\n",
    "                                                                                       'brightness':0.1,\n",
    "                                                                                       'blur':0.1,\n",
    "                                                                                       'cutout':0.1}):\n",
    "    \"\"\"\n",
    "    Build an object detection pipeline supporting multiple annotation formats\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path: Path to images\n",
    "    - labels_path: Path to annotations. For 'coco' and 'edge_impulse_json', this is the path to the single JSON file.\n",
    "    - annotation_format: One of \"yolo\", \"pascal_voc\", \"coco\", \"csv\", \"edge_impulse_json\"\n",
    "    - img_size: Image dimensions (height, width)\n",
    "    - batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "    - train_ds: Training dataset\n",
    "    - val_ds: Validation dataset\n",
    "    - class_mapping: Dictionary mapping class IDs to class names\n",
    "    \"\"\"\n",
    "    if img_size == None:\n",
    "        #infer size from first image\n",
    "        img = Image.open(os.path.join(img_path,os.listdir(img_path)[0]))\n",
    "        img_size = img.size\n",
    "    \n",
    "    if isinstance(augment_strength, float):\n",
    "        #set float values to all augmentations\n",
    "        strength = augment_strength\n",
    "        augment_strength= {'rotation':strength,\n",
    "                           'shear':strength,\n",
    "                           'hue':strength,\n",
    "                           'saturation':strength,\n",
    "                           'contrast':strength,\n",
    "                           'brightness':strength,\n",
    "                           'blur':strength,\n",
    "                           'cutout':strength}\n",
    "        \n",
    "    # YOLO format parser (original)\n",
    "    def parse_yolo_labels(label_path):\n",
    "        label = open(label_path, 'r').read().split('\\n')\n",
    "        \n",
    "        class_ids = []\n",
    "        bbs = []\n",
    "        for line in label:\n",
    "            if line == '':\n",
    "                continue\n",
    "            #first string before space is the class, after that is the bounding box\n",
    "            class_id = int(line.split(' ')[0])\n",
    "            # Check if class_id is a number or string\n",
    "            #read bounding box\n",
    "            bb = line.split(' ')[1:]\n",
    "            bb = [float(i) for i in bb]\n",
    "            #clip values to 0 and 1\n",
    "            bb = [max(0,i) for i in bb]\n",
    "            bb = [min(1,i) for i in bb]\n",
    "            \n",
    "            # YOLO format is x_center, y_center, width, height\n",
    "            # Convert to x1, y1, x2, y2 format\n",
    "            x_center, y_center, width, height = bb\n",
    "            x1 = (x_center - width/2) * img_size[0]\n",
    "            y1 = (y_center - height/2) * img_size[1]\n",
    "            x2 = (x_center + width/2) * img_size[0]\n",
    "            y2 = (y_center + height/2) * img_size[1]\n",
    "            \n",
    "            class_ids.append(class_id)\n",
    "            bbs.append([x1, y1, x2, y2])\n",
    "            \n",
    "        return class_ids, bbs\n",
    "    \n",
    "    # Pascal VOC format parser\n",
    "    def parse_pascal_voc_labels(xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        class_ids = []\n",
    "        bbs = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            class_id = obj.find('name').text\n",
    "            bbox = obj.find('bndbox')\n",
    "            x1 = float(bbox.find('xmin').text)\n",
    "            y1 = float(bbox.find('ymin').text)\n",
    "            x2 = float(bbox.find('xmax').text)\n",
    "            y2 = float(bbox.find('ymax').text)\n",
    "            \n",
    "            class_ids.append(class_id)\n",
    "            bbs.append([x1, y1, x2, y2])\n",
    "            \n",
    "        return class_ids, bbs\n",
    "    \n",
    "    # COCO format parser\n",
    "    def parse_coco_labels(img_file, coco_json_path):\n",
    "        # Extract image filename\n",
    "        img_filename = os.path.basename(img_file)\n",
    "        \n",
    "        # Load COCO JSON\n",
    "        with open(coco_json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        \n",
    "        # Find image ID\n",
    "        img_id = None\n",
    "        for img in coco_data['images']:\n",
    "            if img['file_name'] == img_filename:\n",
    "                img_id = img['id']\n",
    "                img_width = img['width']\n",
    "                img_height = img['height']\n",
    "                break\n",
    "        \n",
    "        if img_id is None:\n",
    "            return [], []\n",
    "        \n",
    "        class_ids = []\n",
    "        bbs = []\n",
    "        \n",
    "        # Find annotations for this image\n",
    "        for ann in coco_data['annotations']:\n",
    "            if ann['image_id'] == img_id:\n",
    "                # COCO uses category_id\n",
    "                category_id = ann['category_id']\n",
    "                \n",
    "                # Map COCO category to our class list\n",
    "                # We need to find the category name first\n",
    "                category_name = None\n",
    "                for cat in coco_data['categories']:\n",
    "                    if cat['id'] == category_id:\n",
    "                        category_name = cat['name']\n",
    "                        break\n",
    "                \n",
    "                class_id = category_name\n",
    "    \n",
    "                # COCO format: [x, y, width, height]\n",
    "                x, y, width, height = ann['bbox']\n",
    "                \n",
    "                # Convert to x1, y1, x2, y2\n",
    "                x1 = x * img_size[0] / img_width\n",
    "                y1 = y * img_size[1] / img_height\n",
    "                x2 = (x + width) * img_size[0] / img_width\n",
    "                y2 = (y + height) * img_size[1] / img_height\n",
    "                \n",
    "                class_ids.append(class_id)\n",
    "                bbs.append([x1, y1, x2, y2])\n",
    "        \n",
    "        return class_ids, bbs\n",
    "    \n",
    "    # CSV format parser\n",
    "    def parse_csv_labels(csv_file, img_file):\n",
    "        img_filename = os.path.basename(img_file)\n",
    "        \n",
    "        class_ids = []\n",
    "        bbs = []\n",
    "        \n",
    "        with open(csv_file, 'r') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            # Skip header if exists\n",
    "            header = next(csv_reader, None)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                # CSV format can vary, but typically:\n",
    "                # filename, class, xmin, ymin, xmax, ymax\n",
    "                if len(row) >= 6 and row[0] == img_filename:\n",
    "                    class_id = row[1]\n",
    "                    \n",
    "                    # Parse coordinates\n",
    "                    xmin, ymin, xmax, ymax = map(float, row[2:6])\n",
    "                    \n",
    "                    # Scale to our image size\n",
    "                    x1 = xmin * img_size[0]\n",
    "                    y1 = ymin * img_size[1]\n",
    "                    x2 = xmax * img_size[0]\n",
    "                    y2 = ymax * img_size[1]\n",
    "                    \n",
    "                    class_ids.append(class_id)\n",
    "                    bbs.append([x1, y1, x2, y2])\n",
    "        \n",
    "        return class_ids, bbs\n",
    "\n",
    "    # Edge Impulse JSON format parser\n",
    "    def parse_edge_impulse_labels(img_file, ei_data):\n",
    "        img_filename = os.path.basename(img_file)\n",
    "        class_ids = []\n",
    "        bbs = []\n",
    "\n",
    "        # Check if the image file exists as a key in the boundingBoxes dictionary\n",
    "        if img_filename in ei_data.get('boundingBoxes', {}):\n",
    "            annotations = ei_data['boundingBoxes'][img_filename]\n",
    "            \n",
    "            for ann in annotations:\n",
    "                label = ann['label']\n",
    "                x = ann['x']\n",
    "                y = ann['y']\n",
    "                width = ann['width']\n",
    "                height = ann['height']\n",
    "                \n",
    "                # Convert to x1, y1, x2, y2 format\n",
    "                x1 = float(x)\n",
    "                y1 = float(y)\n",
    "                x2 = float(x + width)\n",
    "                y2 = float(y + height)\n",
    "                \n",
    "                class_ids.append(label)\n",
    "                bbs.append([x1, y1, x2, y2])\n",
    "        # else: # Handle case where image might not be in the json, though typically it should be\n",
    "        #     print(f\"Warning: Image file {img_filename} not found in Edge Impulse JSON labels.\")\n",
    "            \n",
    "        return class_ids, bbs\n",
    "\n",
    "    # Validate required paths\n",
    "    if img_path is None or labels_path is None:\n",
    "        raise ValueError(\"Both img_path and labels_path must be provided\")\n",
    "    \n",
    "    # Get image files\n",
    "    all_files_in_img_path = sorted(os.listdir(img_path))\n",
    "    # Filter out potential non-image files if necessary, e.g., based on extension\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'} \n",
    "    files = [os.path.join(img_path, f) for f in all_files_in_img_path \n",
    "             if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No valid image files found in {img_path}\")\n",
    "\n",
    "    # Infer img_size from the first valid image file if not provided\n",
    "    if img_size is None:\n",
    "        try:\n",
    "            with Image.open(files[0]) as img:\n",
    "                # Note: PIL returns (width, height), but KerasCV often uses (height, width)\n",
    "                # Let's keep it as (width, height) from PIL for now, and assume downstream handles it.\n",
    "                # Or explicitly swap: img_size = (img.height, img.width) if needed elsewhere.\n",
    "                img_size = img.size \n",
    "            print(f\"Inferred image size (width, height): {img_size}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not open the first image {files[0]} to infer size: {e}\")\n",
    "            \n",
    "    class_ids_all = []\n",
    "    bbs_all = []\n",
    "    \n",
    "    # Parse annotations based on format\n",
    "    annotation_format_lower = annotation_format.lower()\n",
    "    \n",
    "    if annotation_format_lower == \"yolo\":\n",
    "        label_files = []\n",
    "        corresponding_files = []\n",
    "        # Match image files with label files (assuming same base name, different extension)\n",
    "        for img_file in files:\n",
    "            base_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "            potential_label_path = os.path.join(labels_path, base_name + \".txt\")\n",
    "            if os.path.exists(potential_label_path):\n",
    "                label_files.append(potential_label_path)\n",
    "                corresponding_files.append(img_file)\n",
    "            # else:\n",
    "            #     print(f\"Warning: No corresponding YOLO label found for {img_file}\")\n",
    "\n",
    "        files = corresponding_files # Update files list to only include those with labels\n",
    "        print(f\"Processing {len(files)} images with corresponding YOLO labels.\")\n",
    "        for label_file in tqdm(label_files, desc=\"Parsing YOLO labels\"):\n",
    "            class_id, bb = parse_yolo_labels(label_file)\n",
    "            class_ids_all.append(class_id)\n",
    "            bbs_all.append(bb)\n",
    "            \n",
    "    elif annotation_format_lower == \"pascal_voc\":\n",
    "        xml_files = []\n",
    "        corresponding_files = []\n",
    "        # Match image files with xml files\n",
    "        for img_file in files:\n",
    "            base_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "            potential_xml_path = os.path.join(labels_path, base_name + \".xml\")\n",
    "            if os.path.exists(potential_xml_path):\n",
    "                xml_files.append(potential_xml_path)\n",
    "                corresponding_files.append(img_file)\n",
    "            # else:\n",
    "            #     print(f\"Warning: No corresponding Pascal VOC label found for {img_file}\")\n",
    "                \n",
    "        files = corresponding_files # Update files list\n",
    "        print(f\"Processing {len(files)} images with corresponding Pascal VOC labels.\")\n",
    "        for xml_file in tqdm(xml_files, desc=\"Parsing Pascal VOC labels\"):\n",
    "            class_id, bb = parse_pascal_voc_labels(xml_file)\n",
    "            class_ids_all.append(class_id)\n",
    "            bbs_all.append(bb)\n",
    "            \n",
    "    elif annotation_format_lower == \"coco\":\n",
    "        # For COCO, labels_path should be the path to the single JSON file\n",
    "        print(f\"Loading COCO annotations from: {labels_path}\")\n",
    "        try:\n",
    "            with open(labels_path, 'r') as f:\n",
    "                coco_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading COCO JSON file {labels_path}: {e}\")\n",
    "            \n",
    "        print(f\"Processing {len(files)} images using COCO annotations.\")\n",
    "        for img_file in tqdm(files, desc=\"Parsing COCO labels\"):\n",
    "            class_id, bb = parse_coco_labels(img_file, coco_data) # Pass loaded data\n",
    "            class_ids_all.append(class_id)\n",
    "            bbs_all.append(bb)\n",
    "            \n",
    "    elif annotation_format_lower == \"csv\":\n",
    "        # For CSV, labels_path should be the path to the single CSV file\n",
    "        print(f\"Processing {len(files)} images using CSV annotations from: {labels_path}\")\n",
    "        for img_file in tqdm(files, desc=\"Parsing CSV labels\"):\n",
    "            class_id, bb = parse_csv_labels(labels_path, img_file)\n",
    "            class_ids_all.append(class_id)\n",
    "            bbs_all.append(bb)\n",
    "            \n",
    "    elif annotation_format_lower == \"edge_impulse_json\":\n",
    "        # For Edge Impulse JSON, labels_path is the path to the single JSON file\n",
    "        print(f\"Loading Edge Impulse annotations from: {labels_path}\")\n",
    "        try:\n",
    "            with open(labels_path, 'r') as f:\n",
    "                ei_data = json.load(f)\n",
    "                # Basic validation of the loaded structure\n",
    "                if \"boundingBoxes\" not in ei_data:\n",
    "                     raise ValueError(\"Edge Impulse JSON missing 'boundingBoxes' key.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading or parsing Edge Impulse JSON file {labels_path}: {e}\")\n",
    "\n",
    "        print(f\"Processing {len(files)} images using Edge Impulse JSON annotations.\")\n",
    "        valid_files_ei = []\n",
    "        for img_file in tqdm(files, desc=\"Parsing Edge Impulse labels\"):\n",
    "            # Check if the image filename exists in the JSON before parsing\n",
    "            img_filename = os.path.basename(img_file)\n",
    "            if img_filename in ei_data.get('boundingBoxes', {}):\n",
    "                class_id, bb = parse_edge_impulse_labels(img_file, ei_data)\n",
    "                class_ids_all.append(class_id)\n",
    "                bbs_all.append(bb)\n",
    "                valid_files_ei.append(img_file) # Keep track of files that had entries\n",
    "            # else:\n",
    "            #     print(f\"Warning: Image file {img_filename} not found in Edge Impulse JSON labels. Skipping.\")\n",
    "        \n",
    "        # Update the list of files to only include those found in the JSON\n",
    "        files = valid_files_ei \n",
    "        if not files:\n",
    "             raise ValueError(\"No image files from the directory were found in the Edge Impulse JSON labels file.\")\n",
    "        print(f\"Found annotations for {len(files)} images in the Edge Impulse JSON.\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported annotation format: {annotation_format}. \" +\n",
    "                         \"Supported formats: yolo, pascal_voc, coco, csv, edge_impulse_json\")\n",
    "    \n",
    "    \n",
    "    def parse_images(image_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_png(image,channels=3) \n",
    "        #image = tf.image.resize(image, img_size)\n",
    "        return image\n",
    "    \n",
    "    def load_dataset(img_file, id, bb):\n",
    "        image = parse_images(img_file)\n",
    "        #bb to normal tensor\n",
    "        bounding_boxes = {\"boxes\": tf.cast(bb, dtype=tf.float32), \"classes\": tf.cast(id, dtype=tf.float32)}\n",
    "        bounding_boxes = keras_cv.bounding_box.to_dense(bounding_boxes)\n",
    "        return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
    "    \n",
    "    #sort class_ids alphabetically\n",
    "    #create class mapping from class_ids string -> int\n",
    "    class_mapping = {}\n",
    "    for class_id in class_ids_all:\n",
    "        for id in class_id:\n",
    "            if id not in class_mapping:\n",
    "                class_mapping[id] = len(class_mapping)\n",
    "    # for i in range(len(class_mapping.unique())):\n",
    "    #     class_mapping[i] = class_ids[i]\n",
    "    \n",
    "        \n",
    "        \n",
    "    print('class_mapping:',class_mapping)\n",
    "    class_ids = [[class_mapping[id] for id in ids] for ids in class_ids_all]\n",
    "    class_ids = tf.ragged.constant(class_ids)\n",
    "    bbs = tf.ragged.constant(bbs_all)\n",
    "    print('bbs:',bbs[:5])\n",
    "    print('class_ids:',class_ids[:5])\n",
    "    files = tf.ragged.constant(files)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((files, class_ids, bbs))\n",
    "    dataset = dataset.shuffle(len(files))\n",
    "\n",
    "    train_size = int(0.7 * len(files))\n",
    "    val_size = int(0.3 * len(files))\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size)\n",
    "    \n",
    "    augmentor = keras_cv.layers.Augmenter(\n",
    "        [   keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "            #keras_cv.layers.RandomRotation( fill_mode=\"constant\", bounding_box_format=\"xyxy\", factor=augment_strength['rotation']),\n",
    "            keras_cv.layers.RandomShear(bounding_box_format=\"xyxy\",x_factor=augment_strength['shear'],y_factor=augment_strength['shear']),\n",
    "            keras_cv.layers.RandomHue(value_range=(0,255),factor=augment_strength['hue']),\n",
    "            keras_cv.layers.RandomSaturation(factor=(max(0.5-augment_strength['saturation'],0.0),min(0.5+augment_strength['saturation'],1.0))),\n",
    "            keras_cv.layers.RandomContrast(value_range=(0,255),factor=augment_strength['contrast']),\n",
    "            keras_cv.layers.RandomBrightness(value_range=(0,255),factor=augment_strength['brightness']),\n",
    "            keras_cv.layers.RandomGaussianBlur(kernel_size=3, factor=augment_strength['blur']),\n",
    "            keras_cv.layers.RandomCutout(height_factor=augment_strength['cutout'],width_factor=augment_strength['cutout'],fill_mode=\"gaussian_noise\"),\n",
    "            keras_cv.layers.Resizing(height=img_size[0], width=img_size[1], pad_to_aspect_ratio=True, bounding_box_format=\"xyxy\",),\n",
    "            keras_cv.layers.JitteredResize(target_size=(img_size[0],img_size[1]), scale_factor=(0.7, 1.3), bounding_box_format=\"xyxy\"),\n",
    "            #keras_cv.layers.Rescaling(scale=1./255)\n",
    "        ]\n",
    "    )\n",
    "    resizing = keras.Sequential(layers=[keras_cv.layers.Resizing(height=img_size[0], width=img_size[1], pad_to_aspect_ratio=True, bounding_box_format=\"xyxy\"),\n",
    "                                        keras_cv.layers.JitteredResize(target_size=(img_size[0],img_size[1]),scale_factor=(0.75, 1.3),bounding_box_format=\"xyxy\"),\n",
    "                                        #keras_cv.layers.Rescaling(scale=1./255)\n",
    "                                        ]\n",
    "                                )\n",
    "\n",
    "    \n",
    "    train_ds = train_ds.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.ragged_batch(batch_size, drop_remainder=True)\n",
    "    train_ds = train_ds.map(augmentor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    val_ds = val_ds.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.ragged_batch(batch_size, drop_remainder=True)\n",
    "    val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def visualize_dataset(inputs, rows, cols):\n",
    "        #take number of batches to fit rows*cols images\n",
    "        batches_to_take = rows*cols//batch_size\n",
    "        inputs = next(iter(inputs.take(batches_to_take)))\n",
    "        images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "        num_images_in_batch = tf.shape(images)[0]\n",
    "        print(num_images_in_batch)\n",
    "        print(images.shape)\n",
    "        plot_cols = min(num_images_in_batch.numpy(), 5)\n",
    "        plot_rows = num_images_in_batch.numpy() // plot_cols\n",
    "        print(f\"  Displaying images in a {plot_rows}x{plot_cols} grid.\")\n",
    "        id_to_label_mapping = {v: k for k, v in class_mapping.items()}\n",
    "        id_to_str_id_mapping = {int_id: str(int_id) for int_id in class_mapping.values()}\n",
    "        keras_cv.visualization.plot_bounding_box_gallery(\n",
    "            images,\n",
    "            value_range=(0,255),\n",
    "            rows=plot_rows,\n",
    "            cols=plot_cols,\n",
    "            y_true=bounding_boxes,\n",
    "            scale=5,\n",
    "            font_scale=0.5,\n",
    "            bounding_box_format='xyxy',\n",
    "            class_mapping=id_to_str_id_mapping,\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    #rows = max(min(batch_size // 4, 4),1)\n",
    "    \n",
    "    print(\"Training dataset:\")\n",
    "    visualize_dataset(train_ds, 4, 2)\n",
    "    print(\"Validation dataset:\")\n",
    "    visualize_dataset(val_ds, 4, 2)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    def dict_to_tuple(inputs):\n",
    "        return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n",
    "            inputs[\"bounding_boxes\"], max_boxes=32\n",
    "        )\n",
    "    \n",
    "    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    #sort class_mapping by value\n",
    "    class_mapping = dict(sorted(class_mapping.items(), key=lambda item: item[0]))\n",
    "    for i in range(len(class_mapping)):\n",
    "        #find key where value is i\n",
    "        for key, value in class_mapping.items():\n",
    "            if value == i:\n",
    "                print(f\"Class {i}: {key}\")\n",
    "        \n",
    "    \n",
    "    return train_ds, val_ds, class_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'../Data/Trainingsbilder_norbert/all'\n",
    "label_path= r'../Data/Trainingsbilder_norbert/all/bounding_boxes.labels'\n",
    "#train_ds, val_ds, test_ds = build_img_dataset_pipeline(img_path, label_path, n_classes=20, img_size=(416, 416, 3), batch_size=16,img_type='jpeg', label_format='pascal_voc', return_filenames=False)\n",
    "\n",
    "train_ds, val_ds, class_mapping = build_image_detection_pipeline(img_path=img_path, labels_path=label_path, annotation_format=\"edge_impulse_json\",img_size=(128, 128), batch_size=8, augment_strength=0.3)\n",
    "print('num classes:',len(class_mapping))\n",
    "#keras_cv.visualization.plot_image_gallery(train_ds, scale=3,value_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras_cv\n",
    "import keras_hub\n",
    "from keras_hub.models import RetinaNetObjectDetector\n",
    "import traceback # Make sure traceback is imported\n",
    "\n",
    "# --- Revised get_backbone (Uses specific classes) ---\n",
    "def get_backbone(backbone_type, weights, model_size):\n",
    "    \"\"\"\n",
    "    Retrieves a KerasCV backbone instance by selecting the correct class and using its specific methods.\n",
    "    Includes fallback logic.\n",
    "    \"\"\"\n",
    "    assert backbone_type in ['resnetv2', 'cspdarknet', 'efficientnet', 'efficientnetlite', 'mobilenet', 'yolov8'], f\"Backbone {backbone_type} not supported\"\n",
    "    assert weights in ['imagenet', 'coco', 'voc', None], f\"Weights {weights} not supported\"\n",
    "    assert model_size in ['s', 'm', 'l', 'xl'], f\"Model size {model_size} not supported\"\n",
    "\n",
    "    # --- Map generic size to specific model size names ---\n",
    "    size_map = {\n",
    "        'resnetv2':       {'s': '50', 'm': '50', 'l': '101', 'xl': '152'}, # Specific presets handle 18, 34 etc.\n",
    "        'cspdarknet':     {'s': 'tiny', 'm': 's', 'l': 'm', 'xl': 'l'},\n",
    "        'efficientnet':   {'s': 'b0', 'm': 'b1', 'l': 'b2', 'xl': 'b3'}, # Maps to EfficientNetV2\n",
    "        'efficientnetlite':{'s': 'b0', 'm': 'b1', 'l': 'b2', 'xl': 'b3'},\n",
    "        'mobilenet':      {'s': 'small', 'm': 'large', 'l': 'large', 'xl': 'large'}, # Maps to MobileNetV3\n",
    "        'yolov8':         {'s': 'xs', 'm': 's', 'l': 'm', 'xl': 'l'},\n",
    "    }\n",
    "\n",
    "    # --- Map backbone_type string to the actual KerasCV Backbone Class ---\n",
    "    backbone_cls_map = {\n",
    "        # Use the base class; presets will determine the specific ResNetV2 size\n",
    "        'resnetv2': keras_cv.models.ResNetV2Backbone,\n",
    "        'cspdarknet': keras_cv.models.CSPDarkNetBackbone,\n",
    "        'efficientnet': keras_cv.models.EfficientNetV2Backbone, # Defaulting to V2\n",
    "        # EfficientNetLite needs size-specific classes for direct instantiation (if weights=None)\n",
    "        'efficientnetlite': {\n",
    "            'b0': keras_cv.models.EfficientNetLiteB0Backbone,\n",
    "            'b1': keras_cv.models.EfficientNetLiteB1Backbone,\n",
    "            'b2': keras_cv.models.EfficientNetLiteB2Backbone,\n",
    "            'b3': keras_cv.models.EfficientNetLiteB3Backbone,\n",
    "            'b4': keras_cv.models.EfficientNetLiteB4Backbone, # Add B4 if available and needed\n",
    "        },\n",
    "        'mobilenet': keras_cv.models.MobileNetV3Backbone, # Defaulting to V3\n",
    "        'yolov8': keras_cv.models.YOLOV8Backbone,\n",
    "    }\n",
    "\n",
    "    specific_size = size_map.get(backbone_type, {}).get(model_size, None)\n",
    "    if not specific_size:\n",
    "        print(f\"    Warning: Could not map size '{model_size}' for type '{backbone_type}'. Using default 'm' mapping.\")\n",
    "        specific_size = size_map.get(backbone_type, {}).get('m', None)\n",
    "\n",
    "    if not specific_size:\n",
    "        print(f\"    ERROR: Could not determine specific size for {backbone_type}/{model_size}. Falling back.\")\n",
    "        try:\n",
    "            # Use a known working preset as fallback if possible\n",
    "            return keras_cv.models.ResNet50V2Backbone.from_preset(\"resnet50_v2\")\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"    ERROR creating fallback ResNet50V2Backbone: {fallback_e}\")\n",
    "            return None\n",
    "\n",
    "    # Get the specific backbone class (handle EfficientNetLite case)\n",
    "    SpecificBackboneClass = None\n",
    "    if backbone_type == 'efficientnetlite':\n",
    "        # Get the specific class (e.g., EfficientNetLiteB0Backbone) from the nested dict\n",
    "        SpecificBackboneClass = backbone_cls_map['efficientnetlite'].get(specific_size)\n",
    "    elif backbone_type == 'resnetv2':\n",
    "         # ResNetV2 uses the base class, presets define the size\n",
    "         SpecificBackboneClass = backbone_cls_map['resnetv2']\n",
    "    else:\n",
    "        # Get the class directly for other types\n",
    "        SpecificBackboneClass = backbone_cls_map.get(backbone_type)\n",
    "\n",
    "    if SpecificBackboneClass is None:\n",
    "        print(f\"    ERROR: Could not find a specific KerasCV class for {backbone_type} (size {specific_size}). Falling back.\")\n",
    "        try:\n",
    "            return keras_cv.models.ResNet50V2Backbone.from_preset(\"resnet50_v2\")\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"    ERROR creating fallback ResNet50V2Backbone: {fallback_e}\")\n",
    "            return None\n",
    "\n",
    "    backbone_instance = None\n",
    "\n",
    "    # --- Direct Instantiation for weights=None ---\n",
    "    if weights is None:\n",
    "        print(f\"    Instantiating {SpecificBackboneClass.__name__} directly (no weights).\")\n",
    "        try:\n",
    "            # Add specific arguments if needed, e.g., include_rescaling\n",
    "            # Important: Check the __init__ signature of the specific backbone class\n",
    "            kwargs = {}\n",
    "            if 'Lite' in SpecificBackboneClass.__name__: # Heuristic for EfficientNetLite\n",
    "                 # EfficientNetLite backbones often expect include_rescaling\n",
    "                 kwargs['include_rescaling'] = False # Set appropriately based on expected input\n",
    "\n",
    "            # Handle ResNetV2 case where direct instantiation might need size info if not preset\n",
    "            # However, for weights=None, a default ResNet might be okay, or we rely on preset below.\n",
    "            # Sticking to direct instantiation for None weights for consistency.\n",
    "            # If direct `ResNetV2Backbone()` fails, the fallback will catch it.\n",
    "            backbone_instance = SpecificBackboneClass(**kwargs)\n",
    "            print(f\"    Successfully instantiated {SpecificBackboneClass.__name__} (weights=None).\")\n",
    "        except Exception as e_instantiate:\n",
    "            print(f\"    ERROR during direct instantiation of {SpecificBackboneClass.__name__}: {type(e_instantiate).__name__} - {str(e_instantiate)}\")\n",
    "            # Fall through to fallback logic at the end\n",
    "\n",
    "    # --- Preset Loading for weights != None ---\n",
    "    else:\n",
    "        # Construct potential preset names\n",
    "        base_preset = None\n",
    "        weighted_preset = None\n",
    "        preset_to_try = None\n",
    "\n",
    "        # Use specific_size to construct the preset names\n",
    "        if backbone_type == 'resnetv2':\n",
    "             # Map generic size to specific ResNetV2 preset sizes\n",
    "             resnet_preset_size_map = {'50': '50', '101': '101', '152': '152'} # Add 18, 34 if needed\n",
    "             preset_specific_size = resnet_preset_size_map.get(specific_size, '50') # Default to 50\n",
    "             base_preset = f\"resnet{preset_specific_size}_v2\"\n",
    "        elif backbone_type == 'cspdarknet': base_preset = f\"csp_darknet_{specific_size}\"\n",
    "        elif backbone_type == 'efficientnet': base_preset = f\"efficientnetv2_{specific_size}\"\n",
    "        elif backbone_type == 'efficientnetlite':\n",
    "             # Use the preset name you found works:\n",
    "             base_preset = f\"efficientnetlite_{specific_size}\"\n",
    "             # Note: There might not be standard *weighted* presets for EfficientNetLite backbones\n",
    "        elif backbone_type == 'mobilenet': base_preset = f\"mobilenet_v3_{specific_size}\"\n",
    "        elif backbone_type == 'yolov8': base_preset = f\"yolo_v8_{specific_size}_backbone\"\n",
    "\n",
    "        if base_preset:\n",
    "            # Construct weighted name if applicable\n",
    "            if weights == 'imagenet' and backbone_type != 'efficientnetlite': # EffLite might not have imagenet presets\n",
    "                weighted_preset = f\"{base_preset}_imagenet\"\n",
    "            elif weights == 'coco' and backbone_type == 'yolov8':\n",
    "                weighted_preset = f\"{base_preset}_coco\"\n",
    "            # Add elif for 'voc' if specific backbone classes support voc presets, e.g.,\n",
    "            # elif weights == 'voc' and backbone_type == 'yolov8' and specific_size == 'm':\n",
    "            #    weighted_preset = 'yolo_v8_m_pascalvoc' # Detector preset, not backbone\n",
    "\n",
    "            # Decide which name to try loading (prioritize weighted if exists)\n",
    "            preset_to_try = weighted_preset if weighted_preset else base_preset\n",
    "            # Define a potential fallback preset (e.g., the base if weighted failed)\n",
    "            fallback_preset = base_preset if weighted_preset and base_preset != weighted_preset else None\n",
    "\n",
    "            # Attempt loading the primary preset\n",
    "            if preset_to_try:\n",
    "                print(f\"    Attempting to load preset on {SpecificBackboneClass.__name__}: '{preset_to_try}'\")\n",
    "                try:\n",
    "                    # Use from_preset on the SPECIFIC class\n",
    "                    backbone_instance = SpecificBackboneClass.from_preset(preset_to_try)\n",
    "                    print(f\"    Successfully loaded preset: '{preset_to_try}'\")\n",
    "                except Exception as e_preset1:\n",
    "                    print(f\"    Info: Failed loading preset '{preset_to_try}' on {SpecificBackboneClass.__name__}: {type(e_preset1).__name__} - {str(e_preset1)}\")\n",
    "                    # Attempt fallback preset if defined and different\n",
    "                    if fallback_preset and fallback_preset != preset_to_try:\n",
    "                         print(f\"    Attempting to load fallback preset on {SpecificBackboneClass.__name__}: '{fallback_preset}'\")\n",
    "                         try:\n",
    "                             backbone_instance = SpecificBackboneClass.from_preset(fallback_preset)\n",
    "                             print(f\"    Successfully loaded fallback preset: '{fallback_preset}'\")\n",
    "                         except Exception as e_preset2:\n",
    "                             print(f\"    Info: Failed loading fallback preset '{fallback_preset}' on {SpecificBackboneClass.__name__}: {type(e_preset2).__name__} - {str(e_preset2)}\")\n",
    "                             backbone_instance = None # Fallback failed\n",
    "                    else:\n",
    "                         backbone_instance = None # No fallback defined or primary failed\n",
    "\n",
    "        else:\n",
    "            print(f\"    Warning: Could not construct a standard preset name for {backbone_type}/{weights}/{specific_size}. No preset loading attempted.\")\n",
    "\n",
    "    # --- Fallback if everything else failed ---\n",
    "    if backbone_instance is None:\n",
    "        print(f\"    ERROR: Could not get backbone for {backbone_type}/{weights}/{model_size} via specific class methods. Falling back to default ResNet50V2Backbone (no weights).\")\n",
    "        try:\n",
    "            # Attempt direct instantiation as the final fallback\n",
    "            backbone_instance = keras_cv.models.ResNet50V2Backbone()\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"    ERROR creating final fallback ResNet50V2Backbone: {fallback_e}\")\n",
    "            backbone_instance = None # Complete failure\n",
    "\n",
    "    return backbone_instance\n",
    "\n",
    "\n",
    "\n",
    "# --- Corrected build_detection_model ---\n",
    "def build_detection_model(n_classes, lr=0.001, model_name='ssd', weights='coco', model_size='s', backbone='standard'):\n",
    "    \"\"\"\n",
    "    Builds a KerasCV object detection model with automatic fallback for problematic configurations.\n",
    "    Returns both the model and the actual configuration used.\n",
    "    \"\"\"\n",
    "    assert model_name in ['retinanet', 'yolov8'], f\"Model {model_name} not supported in this version\"\n",
    "    assert weights in ['coco', 'imagenet', 'voc', None], f\"Weights {weights} not supported\"\n",
    "    assert model_size in ['s', 'm', 'l', 'xl'], f\"Model size {model_size} not supported\"\n",
    "\n",
    "    # Track actual configuration used (for logging/analysis)\n",
    "    actual_config = {\n",
    "        'model_name': model_name,\n",
    "        'weights': weights,\n",
    "        'model_size': model_size,\n",
    "        'backbone': backbone\n",
    "    }\n",
    "\n",
    "    model = None\n",
    "    print(f\"  Building {model_name} (Size: {model_size}, Weights: {weights}, Backbone: {backbone})\")\n",
    "\n",
    "    try:\n",
    "        # --- Handle COCO weights with fallback strategy ---\n",
    "        if weights == 'coco' and backbone == 'standard':\n",
    "            if model_name == 'retinanet':\n",
    "                # RetinaNet COCO preset is broken, try ImageNet instead\n",
    "                print(\"    Info: RetinaNet COCO preset unavailable. Trying ImageNet weights instead.\")\n",
    "                weights = 'imagenet'\n",
    "                actual_config['weights'] = 'imagenet'\n",
    "                actual_config['fallback_reason'] = 'COCO preset unavailable for RetinaNet'\n",
    "            # YOLOv8 can proceed with COCO as it uses backbone presets\n",
    "\n",
    "        # --- Use KerasCV Detector Presets if applicable ---\n",
    "        detector_preset_name = None\n",
    "        if backbone == 'standard':\n",
    "            if model_name == 'retinanet' and weights == 'voc':\n",
    "                detector_preset_name = 'retinanet_resnet50_pascalvoc'\n",
    "            elif model_name == 'yolov8' and weights == 'voc':\n",
    "                if model_size == 'm':\n",
    "                    detector_preset_name = 'yolo_v8_m_pascalvoc'\n",
    "                else:\n",
    "                    print(f\"    Info: YOLOv8 VOC preset only available for size 'm'. Using ImageNet weights instead.\")\n",
    "                    weights = 'imagenet'\n",
    "                    actual_config['weights'] = 'imagenet'\n",
    "                    actual_config['fallback_reason'] = 'VOC preset only available for size m'\n",
    "\n",
    "        if detector_preset_name:\n",
    "            print(f\"    Attempting to load KerasCV detector preset: '{detector_preset_name}'\")\n",
    "            try:\n",
    "                preset_args = {\n",
    "                    \"num_classes\": n_classes,\n",
    "                    \"bounding_box_format\": \"xyxy\"\n",
    "                }\n",
    "                if model_name == 'retinanet':\n",
    "                    model = keras_cv.models.RetinaNet.from_preset(detector_preset_name, **preset_args)\n",
    "                elif model_name == 'yolov8':\n",
    "                    model = keras_cv.models.YOLOV8Detector.from_preset(detector_preset_name, **preset_args)\n",
    "                print(f\"    Successfully loaded detector preset: '{detector_preset_name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR loading detector preset '{detector_preset_name}': {type(e).__name__} - {str(e)}\")\n",
    "                # If preset fails, fall back to ImageNet weights\n",
    "                weights = 'imagenet'\n",
    "                actual_config['weights'] = 'imagenet'\n",
    "                actual_config['fallback_reason'] = f'Preset loading failed: {type(e).__name__}'\n",
    "                model = None\n",
    "\n",
    "        # --- Build from backbone if no preset used or preset failed ---\n",
    "        if model is None:\n",
    "            backbone_type_str = backbone if backbone != 'standard' else {\n",
    "                'retinanet': 'resnetv2',\n",
    "                'yolov8': 'yolov8'\n",
    "            }.get(model_name)\n",
    "\n",
    "            if not backbone_type_str:\n",
    "                print(f\"    ERROR: Cannot determine backbone type for standard {model_name}.\")\n",
    "                return None, actual_config\n",
    "\n",
    "            print(f\"    No detector preset used or applicable. Building from backbone type: '{backbone_type_str}'...\")\n",
    "            model_backbone = get_backbone(backbone_type_str, weights, model_size)\n",
    "\n",
    "            if model_backbone:\n",
    "                print(f\"    Building {model_name} detector with received backbone: {model_backbone.name}\")\n",
    "                try:\n",
    "                    if model_name == 'retinanet':\n",
    "                        model = keras_cv.models.RetinaNet(\n",
    "                            num_classes=n_classes,\n",
    "                            bounding_box_format=\"xyxy\",\n",
    "                            backbone=model_backbone\n",
    "                        )\n",
    "                    elif model_name == 'yolov8':\n",
    "                        model = keras_cv.models.YOLOV8Detector(\n",
    "                            num_classes=n_classes,\n",
    "                            bounding_box_format=\"xyxy\",\n",
    "                            backbone=model_backbone\n",
    "                        )\n",
    "                except Exception as build_e:\n",
    "                    print(f\"    ERROR during detector construction: {type(build_e).__name__} - {str(build_e)}\")\n",
    "                    if weights not in [None, 'imagenet']:\n",
    "                        # If build fails with other weights, try ImageNet\n",
    "                        print(\"    Attempting fallback to ImageNet weights...\")\n",
    "                        weights = 'imagenet'\n",
    "                        actual_config['weights'] = 'imagenet'\n",
    "                        actual_config['fallback_reason'] = f'Detector construction failed: {type(build_e).__name__}'\n",
    "                        # Recursive call with ImageNet weights\n",
    "                        return build_detection_model(n_classes, lr, model_name, 'imagenet', model_size, backbone)\n",
    "                    model = None\n",
    "            else:\n",
    "                print(f\"    ERROR: Failed to get backbone.\")\n",
    "                if weights not in [None, 'imagenet']:\n",
    "                    # If backbone fails with other weights, try ImageNet\n",
    "                    print(\"    Attempting fallback to ImageNet weights...\")\n",
    "                    weights = 'imagenet'\n",
    "                    actual_config['weights'] = 'imagenet'\n",
    "                    actual_config['fallback_reason'] = 'Backbone creation failed'\n",
    "                    # Recursive call with ImageNet weights\n",
    "                    return build_detection_model(n_classes, lr, model_name, 'imagenet', model_size, backbone)\n",
    "                model = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  FATAL ERROR during build_detection_model: {type(e).__name__} - {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        if weights not in [None, 'imagenet']:\n",
    "            # Final fallback to ImageNet weights\n",
    "            print(\"    Attempting final fallback to ImageNet weights...\")\n",
    "            weights = 'imagenet'\n",
    "            actual_config['weights'] = 'imagenet'\n",
    "            actual_config['fallback_reason'] = f'Fatal error: {type(e).__name__}'\n",
    "            # Recursive call with ImageNet weights\n",
    "            return build_detection_model(n_classes, lr, model_name, 'imagenet', model_size, backbone)\n",
    "        model = None\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"  WARNING: Model building returned None for {model_name} with config: {actual_config}\")\n",
    "\n",
    "    return model, actual_config\n",
    "\n",
    "\n",
    "def compile_model(model, optimizer='adam', lr=0.001):\n",
    "    \"\"\"\n",
    "    Compiles the model with appropriate loss functions and optimizer.\n",
    "    \n",
    "    Args:\n",
    "        model: The keras model to compile\n",
    "        optimizer: String name of optimizer or optimizer instance\n",
    "        lr: Learning rate (used only if optimizer is specified as string)\n",
    "    \"\"\"\n",
    "    print(f\"  Compiling model of type: {type(model)}\")\n",
    "    \n",
    "    try:\n",
    "        # If optimizer is a string, create the appropriate optimizer instance\n",
    "        if isinstance(optimizer, str):\n",
    "            optimizer = optimizer.lower()\n",
    "            if optimizer == 'lamb':\n",
    "                try:\n",
    "                    optimizer = keras.optimizers.LAMB(learning_rate=lr)\n",
    "                except (AttributeError, ImportError):\n",
    "                    print(\"    Warning: LAMB optimizer not available, falling back to Adam\")\n",
    "                    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            elif optimizer == 'adam':\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "            elif optimizer == 'rmsprop':\n",
    "                optimizer = keras.optimizers.RMSprop(learning_rate=lr)\n",
    "            elif optimizer == 'sgd':\n",
    "                optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "            else:\n",
    "                print(f\"    Warning: Unknown optimizer {optimizer}, falling back to Adam\")\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        # Set appropriate losses based on model type\n",
    "        if isinstance(model, keras_cv.models.RetinaNet):\n",
    "            print(\"    Detected KerasCV RetinaNet. Setting losses to 'Focal' and 'SmoothL1'.\")\n",
    "            classification_loss = \"focal\"\n",
    "            box_loss = \"smoothl1\"\n",
    "            print(f\"    Using Classification Loss: {classification_loss}, Box Loss: {box_loss}\")\n",
    "            \n",
    "        elif isinstance(model, keras_cv.models.YOLOV8Detector):\n",
    "            print(\"    Using default losses for YOLOV8Detector: Binary Crossentropy and CIoU.\")\n",
    "            classification_loss = \"binary_crossentropy\"\n",
    "            box_loss = \"ciou\"\n",
    "            print(f\"    Using Classification Loss: {classification_loss}, Box Loss: {box_loss}\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {type(model)}\")\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            classification_loss=classification_loss,\n",
    "            box_loss=box_loss\n",
    "        )\n",
    "        print(\"  Model compiled successfully.\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during model compilation: {type(e).__name__}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_detection_models():\n",
    "    \"\"\"\n",
    "    Test function to construct models with various configurations.\n",
    "    Now includes testing of fallback behavior and tracking of actual configurations used.\n",
    "    \"\"\"\n",
    "    n_classes = 20  # For Pascal VOC dataset\n",
    "\n",
    "    # Define test configurations\n",
    "    test_configs = [\n",
    "        # Test COCO fallback behavior\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': None, 'backbone': 'efficientnetlite'},\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': 'coco', 'backbone': 'standard'},  # Should fallback to ImageNet\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'coco', 'backbone': 'standard'},     # Should work with COCO backbone\n",
    "        \n",
    "        # Standard backbone configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 'm', 'weights': 'voc', 'backbone': 'standard'},\n",
    "        {'model_name': 'yolov8', 'model_size': 'm', 'weights': 'voc', 'backbone': 'standard'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'voc', 'backbone': 'standard'},  # Test VOC with non-m size\n",
    "        \n",
    "        # ResNetV2 backbone configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'resnetv2'},\n",
    "        {'model_name': 'retinanet', 'model_size': 'm', 'weights': 'imagenet', 'backbone': 'resnetv2'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'resnetv2'},\n",
    "        \n",
    "        # CSPDarkNet backbone configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'cspdarknet'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'cspdarknet'},\n",
    "        \n",
    "        # EfficientNetV2 backbone configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'efficientnet'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'efficientnet'},\n",
    "        \n",
    "        # Test no-weight configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': None, 'backbone': 'resnetv2'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': None, 'backbone': 'resnetv2'},\n",
    "        \n",
    "        # MobileNetV3 backbone configurations\n",
    "        {'model_name': 'retinanet', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'mobilenet'},\n",
    "        {'model_name': 'yolov8', 'model_size': 's', 'weights': 'imagenet', 'backbone': 'mobilenet'},\n",
    "    ]\n",
    "\n",
    "    print(\"--- Starting Model Configuration Test ---\")\n",
    "    print(f\"Found {len(test_configs)} configurations to test.\")\n",
    "    print(f\"Using n_classes = {n_classes}\")\n",
    "\n",
    "    results = {}\n",
    "    fallbacks = {}  # Track configurations that used fallback\n",
    "    \n",
    "    for i, config in enumerate(test_configs):\n",
    "        model_name = config['model_name']\n",
    "        model_size = config['model_size']\n",
    "        weights = config['weights']\n",
    "        backbone = config['backbone']\n",
    "        \n",
    "        config_str = f\"Config {i+1}/{len(test_configs)}: Model={model_name}, Size={model_size}, Weights={weights}, Backbone={backbone}\"\n",
    "        print(f\"\\nTesting: {config_str}\")\n",
    "        \n",
    "        try:\n",
    "            # Build model with new return value (model and actual_config)\n",
    "            print(\"  Building model...\")\n",
    "            model, actual_config = build_detection_model(\n",
    "                n_classes=n_classes,\n",
    "                model_name=model_name,\n",
    "                weights=weights,\n",
    "                model_size=model_size,\n",
    "                backbone=backbone\n",
    "            )\n",
    "            \n",
    "            if model is None:\n",
    "                raise ValueError(\"build_detection_model returned None\")\n",
    "                \n",
    "            print(f\"  Model built successfully. Type: {type(model)}\")\n",
    "            \n",
    "            # Track if configuration used fallback\n",
    "            if actual_config['weights'] != weights:\n",
    "                fallback_info = {\n",
    "                    'requested_weights': weights,\n",
    "                    'actual_weights': actual_config['weights'],\n",
    "                    'reason': actual_config.get('fallback_reason', 'Unknown')\n",
    "                }\n",
    "                fallbacks[config_str] = fallback_info\n",
    "            \n",
    "            # Compile model\n",
    "            model = compile_model(model)\n",
    "            if model is None:\n",
    "                raise ValueError(\"compile_model returned None\")\n",
    "            \n",
    "            results[config_str] = \"Success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during build/compile: {type(e).__name__}: {e}\")\n",
    "            results[config_str] = f\"Failed ({type(e).__name__}: {str(e)})\"\n",
    "            \n",
    "        finally:\n",
    "            # Clean up\n",
    "            if 'model' in locals() and model is not None:\n",
    "                print(\"  Cleaning up model...\")\n",
    "                del model\n",
    "            gc.collect()\n",
    "            keras.backend.clear_session()\n",
    "            print(f\"  Memory cleared.\")\n",
    "\n",
    "    print(\"\\n--- Model Configuration Test Summary ---\")\n",
    "    successful_configs = 0\n",
    "    for config_str, result in results.items():\n",
    "        print(f\"{config_str}: {result}\")\n",
    "        if result == \"Success\":\n",
    "            successful_configs += 1\n",
    "            # If this config used fallback, print the fallback information\n",
    "            if config_str in fallbacks:\n",
    "                fallback = fallbacks[config_str]\n",
    "                print(f\"   → Fallback used: {fallback['requested_weights']} → {fallback['actual_weights']}\")\n",
    "                print(f\"   → Reason: {fallback['reason']}\")\n",
    "    \n",
    "    print(f\"\\nTotal Successful Configurations: {successful_configs}/{len(test_configs)}\")\n",
    "    \n",
    "    if fallbacks:\n",
    "        print(\"\\n--- Fallback Summary ---\")\n",
    "        print(f\"Total configurations using fallback: {len(fallbacks)}\")\n",
    "        for config_str, fallback in fallbacks.items():\n",
    "            print(f\"\\n{config_str}:\")\n",
    "            print(f\"  Requested weights: {fallback['requested_weights']}\")\n",
    "            print(f\"  Used weights: {fallback['actual_weights']}\")\n",
    "            print(f\"  Reason: {fallback['reason']}\")\n",
    "    \n",
    "    print(\"\\n--- Test Complete ---\")\n",
    "\n",
    "# Run the expanded tests\n",
    "#test_detection_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detection_model(model, train_ds, val_ds, epochs=10, verbose=1,folder='projects',use_wandb=False):\n",
    "    '''\n",
    "    This function trains a model for object detection using Keras CV.\n",
    "    \n",
    "    Parameters:\n",
    "    model: keras.Model, the model for object detection\n",
    "    train_ds: tf.data.Dataset, the training dataset\n",
    "    val_ds: tf.data.Dataset, the validation dataset\n",
    "    epochs: int, number of epochs for training\n",
    "    verbose: int, whether to print training information\n",
    "    \n",
    "    Returns:\n",
    "    history: dict, history of training metrics\n",
    "    '''\n",
    "    save_path = r'../Data/trained_models'\n",
    "    model_name = model.name\n",
    "    if 'yolov8' in model_name:\n",
    "        model_name = 'yolov8'\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    savedir = os.path.join(save_path, folder, datetime_str, model_name)\n",
    "    print('savedir:',savedir)\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        \n",
    "    checkpoints = os.path.join(savedir, 'best_weights')\n",
    "    if not os.path.exists(checkpoints):\n",
    "        os.makedirs(checkpoints, exist_ok=True)\n",
    "\n",
    "    \n",
    "    coco_metrics = keras_cv.metrics.BoxCOCOMetrics(bounding_box_format=\"xyxy\",evaluate_freq=1)\n",
    "    callbacks = []\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoints+'/checkpoint.weights.h5'),save_best_only=True,save_weights_only=True,monitor='val_loss',mode='min',verbose=1))\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True))\n",
    "    callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='min', min_delta=0.001, cooldown=0, min_lr=0))\n",
    "    callbacks.append(keras_cv.callbacks.PyCOCOCallback(val_ds, bounding_box_format=\"xyxy\"))\n",
    "    if use_wandb:\n",
    "        callbacks.append(wandb.keras.WandbCallback(save_model=False))\n",
    "    start_time = time.perf_counter()\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=verbose,callbacks=callbacks)\n",
    "    end_time = time.perf_counter()\n",
    "    training_time = end_time - start_time\n",
    "    #load best weights\n",
    "    model.load_weights(os.path.join(checkpoints+'/checkpoint.weights.h5'))\n",
    "    # Save the model\n",
    "    model.save(os.path.join(savedir, 'model.keras'))\n",
    "    \n",
    "    eval_metrics = evaluate_object_detection_model(model, val_ds)\n",
    "        \n",
    "    # Combine metrics\n",
    "    metrics = {\n",
    "        # Training metrics\n",
    "        'training_time': training_time,\n",
    "        'training_time_per_epoch': training_time / len(history.history['loss']),\n",
    "        'best_epoch': np.argmin(history.history['val_loss']) + 1,\n",
    "        'best_val_loss': min(history.history['val_loss']),\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        \n",
    "        # Evaluation metrics from best model\n",
    "        **eval_metrics,\n",
    "        \n",
    "        # Save paths\n",
    "        'model_dir': savedir,\n",
    "        'best_model_path': os.path.join(savedir, \"best_model\")\n",
    "    }\n",
    "    \n",
    "    # Calculate composite score\n",
    "    metrics['composite_score'] = (\n",
    "        0.4 * metrics['mAP'] + \n",
    "        0.3 * metrics['mAP_50'] + \n",
    "        0.2 * (1.0 - metrics['best_val_loss']) +\n",
    "        0.1 * metrics['mAP_large']\n",
    "    )\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(os.path.join(savedir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    \n",
    "    return metrics, history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, config = build_detection_model(n_classes=len(class_mapping), model_name='yolov8',weights='voc',model_size='s',backbone='standard')\n",
    "print(\"Type returned by build_detection_model:\", type(model)) # Add this line\n",
    "print(\"Value returned by build_detection_model:\", model)    \n",
    "model = compile_model(model, optimizer='adam', lr=0.001)\n",
    "metrics, history, model = train_detection_model(model, train_ds, val_ds, epochs=5, verbose=1, folder='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_object_detection_model(model, dataset, num_inference_batches=100):\n",
    "    \"\"\"\n",
    "    Evaluates an object detection model using COCO metrics and measures inference performance.\n",
    "    \n",
    "    Args:\n",
    "        model: The compiled keras model to evaluate\n",
    "        dataset: Dataset to evaluate on\n",
    "        num_inference_batches: Number of batches to use for inference timing (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics and inference performance metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    \n",
    "    # Initialize metrics\n",
    "    coco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "        bounding_box_format=\"xyxy\",\n",
    "        evaluate_freq=1\n",
    "    )\n",
    "    \n",
    "    inference_times = []\n",
    "    batch_sizes = []\n",
    "    \n",
    "    # Track memory usage\n",
    "    initial_memory = tf.config.experimental.get_memory_info('GPU:0')['current'] if tf.config.list_physical_devices('GPU') else 0\n",
    "    peak_memory = initial_memory\n",
    "    \n",
    "    # Warm-up run\n",
    "    print(\"Performing warm-up inference...\")\n",
    "    for batch in dataset.take(1):\n",
    "        images, _ = batch\n",
    "        model.predict(images, verbose=0)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f\"Starting evaluation on {num_inference_batches} batches...\")\n",
    "    progress_step = max(1, num_inference_batches // 10)\n",
    "    \n",
    "    for i, batch in enumerate(dataset):\n",
    "        images, y_true = batch\n",
    "        batch_sizes.append(len(images))\n",
    "        \n",
    "        # Measure inference time for first num_inference_batches\n",
    "        if i < num_inference_batches:\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(images, verbose=0)\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            if (i + 1) % progress_step == 0:\n",
    "                print(f\"Processed {i + 1}/{num_inference_batches} timing batches \"\n",
    "                      f\"(avg time: {np.mean(inference_times[-progress_step:]):.4f}s)\")\n",
    "        else:\n",
    "            # For remaining batches, only update COCO metrics without timing\n",
    "            y_pred = model.predict(images, verbose=0)\n",
    "        \n",
    "        # Update COCO metrics\n",
    "        coco_metrics.update_state(y_true, y_pred)\n",
    "        \n",
    "        # Track memory\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            current_memory = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
    "            peak_memory = max(peak_memory, current_memory)\n",
    "    \n",
    "    # Calculate COCO metrics\n",
    "    coco_results = coco_metrics.result()\n",
    "    \n",
    "    # Calculate inference metrics\n",
    "    metrics = {\n",
    "        # COCO metrics\n",
    "        'mAP': coco_results['AP'],\n",
    "        'mAP_50': coco_results['AP50'],\n",
    "        'mAP_75': coco_results['AP75'],\n",
    "        'mAP_small': coco_results['APs'],\n",
    "        'mAP_medium': coco_results['APm'],\n",
    "        'mAP_large': coco_results['APl'],\n",
    "        \n",
    "        # Timing metrics\n",
    "        'avg_inference_time': np.mean(inference_times),\n",
    "        'std_inference_time': np.std(inference_times),\n",
    "        'min_inference_time': np.min(inference_times),\n",
    "        'max_inference_time': np.max(inference_times),\n",
    "        'p90_inference_time': np.percentile(inference_times, 90),\n",
    "        'p95_inference_time': np.percentile(inference_times, 95),\n",
    "        'avg_fps': np.mean(batch_sizes) / np.mean(inference_times),\n",
    "        'peak_fps': np.max(batch_sizes) / np.min(inference_times),\n",
    "        \n",
    "        # Memory metrics\n",
    "        'peak_memory_gb': (peak_memory - initial_memory) / 1e9,\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"mAP: {metrics['mAP']:.4f}\")\n",
    "    print(f\"mAP_50: {metrics['mAP_50']:.4f}\")\n",
    "    print(f\"mAP_75: {metrics['mAP_75']:.4f}\")\n",
    "    print(f\"Average inference time: {metrics['avg_inference_time']*1000:.2f} ms\")\n",
    "    print(f\"Average FPS: {metrics['avg_fps']:.2f}\")\n",
    "    print(f\"Peak FPS: {metrics['peak_fps']:.2f}\")\n",
    "    print(f\"Peak memory usage: {metrics['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = evaluate_object_detection_model(model, val_ds)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset,class_mapping):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    num_images_in_batch = tf.shape(images)[0]\n",
    "    y_pred = model.predict(images)\n",
    "    plot_cols = min(num_images_in_batch.numpy(), 5)\n",
    "    plot_rows = num_images_in_batch.numpy() // plot_cols\n",
    "    id_to_str_id_mapping = {int_id: str(int_id) for int_id in class_mapping.values()}\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format='xyxy',\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=3,\n",
    "        rows=plot_rows,\n",
    "        cols=plot_cols,\n",
    "        show=True,\n",
    "        font_scale=0.5,\n",
    "        class_mapping=id_to_str_id_mapping,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "#visualize_detections(model, dataset=val_ds, bounding_box_format=\"xyxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "wandb_key_ue='0639f18532accef34517cd42931a68968331436e'\n",
    "\n",
    "def find_best_detection_model(train_ds, val_ds, n_classes, \n",
    "                            project=\"object_detection_optimization\",\n",
    "                            max_epochs=60, \n",
    "                            iterations=50,\n",
    "                            models = ['retinanet', 'yolov8'],\n",
    "                            model_sizes = ['s', 'm', 'l'],\n",
    "                            backbones = ['resnetv2', 'cspdarknet', 'efficientnet', 'efficientnetlite', 'mobilenet', 'standard'],\n",
    "                            use_wandb=True,\n",
    "                            wandb_api_key=None):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization to find the best object detection model.\n",
    "    Uses both validation loss and COCO metrics for evaluation.\n",
    "    \"\"\"\n",
    "    if models == 'all':\n",
    "        models = ['retinanet', 'yolov8']\n",
    "    if model_sizes == 'all':\n",
    "        model_sizes = ['s', 'm', 'l']\n",
    "    if backbones == 'all':\n",
    "        backbones = ['resnetv2', 'cspdarknet', 'efficientnet', 'efficientnetlite', 'mobilenet', 'standard']\n",
    "    assert len(models) > 0, \"models must be a list\"\n",
    "    assert len(model_sizes) > 0, \"model_sizes must be a list\"\n",
    "    assert len(backbones) > 0, \"backbones must be a list\"\n",
    "    assert all(model in ['retinanet', 'yolov8'] for model in models), \"all models must be either 'retinanet' or 'yolov8'\"\n",
    "    assert all(model_size in ['s', 'm', 'l'] for model_size in model_sizes), \"all model_sizes must be either 's', 'm', or 'l'\"\n",
    "    assert all(backbone in ['resnetv2', 'cspdarknet', 'efficientnet', 'efficientnetlite', 'mobilenet', 'standard'] for backbone in backbones), \"all backbones must be either 'resnetv2', 'cspdarknet', 'efficientnet', 'efficientnetlite', 'mobilenet', or 'standard'\"\n",
    "    \n",
    "    if use_wandb:\n",
    "        if wandb_api_key:\n",
    "            wandb.login(key=wandb_api_key)\n",
    "        else:\n",
    "            wandb.login()\n",
    "\n",
    "    def HPO_objective(trial):\n",
    "        # Define the search space\n",
    "        hyperparameters = {\n",
    "            # Model Architecture\n",
    "            'model_name': trial.suggest_categorical('model_name', models),\n",
    "            'model_size': trial.suggest_categorical('model_size', model_sizes),\n",
    "            'backbone': trial.suggest_categorical('backbone', backbones),\n",
    "            'weights': trial.suggest_categorical('weights', ['imagenet', 'voc', None]),  # Removed 'coco' as it's handled by fallback\n",
    "            \n",
    "            # Training Parameters\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64]),\n",
    "            'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop','sgd', 'lamb']),\n",
    "            \n",
    "            # Model-specific Parameters\n",
    "            'fpn_depth': trial.suggest_int('fpn_depth', 1, 3),  # Feature Pyramid Network depth\n",
    "            'anchor_scale': trial.suggest_float('anchor_scale', 2, 6),\n",
    "        }\n",
    "\n",
    "        # Initialize WandB run\n",
    "        if use_wandb:\n",
    "            run_name = f\"{hyperparameters['model_name']}_{hyperparameters['backbone']}_{trial.number}\"\n",
    "            run = wandb.init(project=project,\n",
    "                            name=run_name,\n",
    "                            config=hyperparameters,\n",
    "                            group=f\"trial_{trial.number}\",\n",
    "                            reinit=True)\n",
    "\n",
    "\n",
    "        model, actual_config = build_detection_model(\n",
    "            n_classes=n_classes,\n",
    "            model_name=hyperparameters['model_name'],\n",
    "            weights=hyperparameters['weights'],\n",
    "            model_size=hyperparameters['model_size'],\n",
    "            backbone=hyperparameters['backbone']\n",
    "        )\n",
    "        \n",
    "        model = compile_model(model, optimizer=hyperparameters['optimizer'],lr=hyperparameters['learning_rate'])\n",
    "\n",
    "        # Log actual configuration if different from requested\n",
    "        if use_wandb and actual_config['weights'] != hyperparameters['weights']:\n",
    "            wandb.log({'requested_weights': hyperparameters['weights'],\n",
    "                    'actual_weights': actual_config['weights'],\n",
    "                    'fallback_reason': actual_config.get('fallback_reason', 'Unknown')\n",
    "                    })\n",
    "        \n",
    "\n",
    "        # Train model\n",
    "        metrics, history, model = train_detection_model(model, train_ds, val_ds, epochs=max_epochs, verbose=0, folder='HPO')\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        wandb.finish()\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        composite_score = metrics['composite_score']\n",
    "\n",
    "        return composite_score\n",
    "\n",
    "\n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=5,\n",
    "            interval_steps=3\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add custom attributes to study\n",
    "    study.set_user_attr(\"n_classes\", n_classes)\n",
    "    study.set_user_attr(\"max_epochs\", max_epochs)\n",
    "    study.set_user_attr(\"project\", project)\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        HPO_objective,\n",
    "        n_trials=iterations,\n",
    "        timeout=None,  # No timeout\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nStudy Statistics:\")\n",
    "    print(f\"Number of completed trials: {len(study.trials)}\")\n",
    "    print(f\"Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "    print(f\"Number of failed trials: {len(study.get_trials(states=[optuna.trial.TrialState.FAIL]))}\")\n",
    "\n",
    "    print(\"\\nBest Trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Value (composite score): {best_trial.value:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Save study results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f\"optimization_results_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save study statistics\n",
    "    study.trials_dataframe().to_csv(f\"{results_dir}/study_results.csv\")\n",
    "    \n",
    "    # Create visualization plots\n",
    "    try:\n",
    "        fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "        fig2 = optuna.visualization.plot_param_importances(study)\n",
    "        fig3 = optuna.visualization.plot_parallel_coordinate(study)\n",
    "        \n",
    "        # Save plots if possible\n",
    "        if hasattr(fig1, 'write_image'):\n",
    "            fig1.write_image(f\"{results_dir}/optimization_history.png\")\n",
    "            fig2.write_image(f\"{results_dir}/param_importances.png\")\n",
    "            fig3.write_image(f\"{results_dir}/parallel_coordinate.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create visualization plots: {e}\")\n",
    "\n",
    "    return study, best_trial.params, results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'../Data/voc2007/all/images'\n",
    "label_path= r'../Data/voc2007/all/annotations'\n",
    "train_ds, val_ds, class_mapping = build_image_detection_pipeline(img_path=img_path, labels_path=label_path, annotation_format=\"pascal_voc\",img_size=(128,128), batch_size=32)\n",
    "print('len class mapping: ',len(class_mapping))\n",
    "\n",
    "find_best_detection_model(train_ds, val_ds, n_classes=20, models='all', model_sizes=['s','m'],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
